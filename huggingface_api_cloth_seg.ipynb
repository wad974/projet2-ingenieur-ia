{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_cell"
      },
      "source": [
        "# Guide pour l'Appel à une API Hugging Face pour la Segmentation d'Images\n",
        "\n",
        "Bienvenue ! Ce notebook a pour but de vous guider pas à pas dans l'utilisation de l'API d'inférence de Hugging Face pour effectuer de la segmentation d'images. La segmentation d'images consiste à attribuer une étiquette (comme \"cheveux\", \"vêtement\", \"arrière-plan\") à chaque pixel d'une image.\n",
        "\n",
        "Nous allons :\n",
        "1. Comprendre ce qu'est une API et comment s'y connecter.\n",
        "2. Envoyer une image à un modèle de segmentation hébergé sur Hugging Face.\n",
        "3. Récupérer et interpréter les résultats.\n",
        "4. Visualiser les masques de segmentation.\n",
        "5. Étendre cela pour traiter plusieurs images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports_intro_cell"
      },
      "source": [
        "## 1. Configuration Initiale et Importations\n",
        "\n",
        "Commençons par importer les bibliothèques Python nécessaires. Nous aurons besoin de :\n",
        "- `os` pour interagir avec le système de fichiers (lister les images).\n",
        "- `requests` pour effectuer des requêtes HTTP vers l'API.\n",
        "- `PIL (Pillow)` pour manipuler les images.\n",
        "- `matplotlib.pyplot` pour afficher les images et les masques.\n",
        "- `numpy` pour la manipulation des tableaux (les images sont des tableaux de pixels).\n",
        "- `tqdm.notebook` pour afficher une barre de progression (utile pour plusieurs images).\n",
        "- `base64` et `io` pour décoder les masques renvoyés par l'API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports_code_cell"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import base64\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_vars_intro_cell"
      },
      "source": [
        "### Variables de Configuration\n",
        "\n",
        "Nous devons définir quelques variables :\n",
        "- `image_dir`: Le chemin vers le dossier contenant vos images. **Assurez-vous de modifier ce chemin si nécessaire.**\n",
        "- `max_images`: Le nombre maximum d'images à traiter (pour ne pas surcharger l'API ou attendre trop longtemps).\n",
        "- `api_token`: Votre jeton d'API Hugging Face. **IMPORTANT : Gardez ce jeton secret !**\n",
        "\n",
        "**Comment obtenir un token API Hugging Face ?**\n",
        "1. Créez un compte sur [huggingface.co](https://huggingface.co/).\n",
        "2. Allez dans votre profil -> Settings -> Access Tokens.\n",
        "3. Créez un nouveau token (par exemple, avec le rôle \"read\").\n",
        "4. Copiez ce token ici."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config_vars_code_cell"
      },
      "outputs": [],
      "source": [
        "# TODO: Modifiez ces valeurs selon votre configuration\n",
        "image_dir = \"/content/images_a_segmenter\"  # Exemple : si vous êtes sur Colab et avez uploadé un dossier\n",
        "max_images = 3  # Commençons avec peu d'images\n",
        "\n",
        "# IMPORTANT: Remplacez \"VOTRE_TOKEN_HUGGING_FACE_ICI\" par votre véritable token API.\n",
        "# Ne partagez jamais votre token publiquement.\n",
        "api_token = \"VOTRE_TOKEN_HUGGING_FACE_ICI\"\n",
        "\n",
        "# Créons le dossier d'images s'il n'existe pas (pour l'exemple)\n",
        "if not os.path.exists(image_dir):\n",
        "    os.makedirs(image_dir)\n",
        "    print(f\"Dossier '{image_dir}' créé. Veuillez y ajouter des images .jpg ou .png.\")\n",
        "else:\n",
        "    print(f\"Dossier '{image_dir}' existant.\")\n",
        "\n",
        "if api_token == \"VOTRE_TOKEN_HUGGING_FACE_ICI\":\n",
        "    print(\"\\nATTENTION : Vous devez remplacer 'VOTRE_TOKEN_HUGGING_FACE_ICI' par votre token API personnel.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "api_understanding_cell"
      },
      "source": [
        "## 2. Comprendre l'API d'Inférence Hugging Face\n",
        "\n",
        "L'API d'inférence permet d'utiliser des modèles hébergés sur Hugging Face sans avoir à les télécharger ou à gérer l'infrastructure.\n",
        "\n",
        "- **Modèle utilisé** : Nous allons utiliser le modèle `sayeed99/segformer_b3_clothes`, spécialisé dans la segmentation de vêtements et de parties du corps.\n",
        "- **URL de l'API** : L'URL pour un modèle est généralement `https://api-inference.huggingface.co/models/NOM_DU_MODELE`.\n",
        "- **Headers (En-têtes)** : Pour s'authentifier et spécifier le type de contenu, nous envoyons des en-têtes avec notre requête.\n",
        "    - `Authorization`: Contient notre token API (précédé de `Bearer `).\n",
        "    - `Content-Type`: Indique que nous envoyons une image au format JPEG (ou PNG selon le cas)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "api_setup_code_cell"
      },
      "outputs": [],
      "source": [
        "API_URL = \"https://api-inference.huggingface.co/models/...\" # Remplacez ... par le bon endpoint.\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_token}\"\n",
        "    # Le \"Content-Type\" sera ajouté dynamiquement lors de l'envoi de l'image\n",
        "}\n",
        "\n",
        "# Lister les chemins des images à traiter\n",
        "# Assurez-vous d'avoir des images dans le dossier 'image_dir'!\n",
        "image_paths = [] # A vous de jouer !\n",
        "\n",
        "\n",
        "if not image_paths:\n",
        "    print(f\"Aucune image trouvée dans '{image_dir}'. Veuillez y ajouter des images.\")\n",
        "else:\n",
        "    print(f\"{len(image_paths)} image(s) à traiter : {image_paths}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "helper_functions_intro_cell"
      },
      "source": [
        "## 3. Fonctions Utilitaires pour le Traitement des Masques\n",
        "\n",
        "Le modèle que nous utilisons (`sayeed99/segformer_b3_clothes`) renvoie des masques pour différentes classes (cheveux, chapeau, etc.). Ces masques sont encodés en base64. Les fonctions ci-dessous sont fournies pour vous aider à :\n",
        "1.  `CLASS_MAPPING`: Un dictionnaire qui associe les noms de classes (ex: \"Hat\") à des identifiants numériques.\n",
        "2.  `get_image_dimensions`: Récupérer les dimensions d'une image.\n",
        "3.  `decode_base64_mask`: Décoder un masque de base64 en une image (tableau NumPy) et le redimensionner.\n",
        "4.  `create_masks`: Combiner les masques de toutes les classes détectées en un seul masque de segmentation final, où chaque pixel a la valeur de l'ID de sa classe.\n",
        "\n",
        "**Cette partie est donnée car elle est spécifique au format de sortie de ce modèle et un peu complexe pour une première approche.** Lisez-la pour comprendre son rôle, mais ne vous attardez pas sur les détails d'implémentation pour l'instant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "helper_functions_code_cell"
      },
      "outputs": [],
      "source": [
        "CLASS_MAPPING = {\n",
        "    \"Background\": 0,\n",
        "    \"Hat\": 1,\n",
        "    \"Hair\": 2,\n",
        "    \"Sunglasses\": 3,\n",
        "    \"Upper-clothes\": 4,\n",
        "    \"Skirt\": 5,\n",
        "    \"Pants\": 6,\n",
        "    \"Dress\": 7,\n",
        "    \"Belt\": 8,\n",
        "    \"Left-shoe\": 9,\n",
        "    \"Right-shoe\": 10,\n",
        "    \"Face\": 11,\n",
        "    \"Left-leg\": 12,\n",
        "    \"Right-leg\": 13,\n",
        "    \"Left-arm\": 14,\n",
        "    \"Right-arm\": 15,\n",
        "    \"Bag\": 16,\n",
        "    \"Scarf\": 17\n",
        "}\n",
        "\n",
        "def get_image_dimensions(img_path):\n",
        "    \"\"\"\n",
        "    Get the dimensions of an image.\n",
        "\n",
        "    Args:\n",
        "        img_path (str): Path to the image.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (width, height) of the image.\n",
        "    \"\"\"\n",
        "    original_image = Image.open(img_path)\n",
        "    return original_image.size\n",
        "\n",
        "def decode_base64_mask(base64_string, width, height):\n",
        "    \"\"\"\n",
        "    Decode a base64-encoded mask into a NumPy array.\n",
        "\n",
        "    Args:\n",
        "        base64_string (str): Base64-encoded mask.\n",
        "        width (int): Target width.\n",
        "        height (int): Target height.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Single-channel mask array.\n",
        "    \"\"\"\n",
        "    mask_data = base64.b64decode(base64_string)\n",
        "    mask_image = Image.open(io.BytesIO(mask_data))\n",
        "    mask_array = np.array(mask_image)\n",
        "    if len(mask_array.shape) == 3:\n",
        "        mask_array = mask_array[:, :, 0]  # Take first channel if RGB\n",
        "    mask_image = Image.fromarray(mask_array).resize((width, height), Image.NEAREST)\n",
        "    return np.array(mask_image)\n",
        "\n",
        "def create_masks(results, width, height):\n",
        "    \"\"\"\n",
        "    Combine multiple class masks into a single segmentation mask.\n",
        "\n",
        "    Args:\n",
        "        results (list): List of dictionaries with 'label' and 'mask' keys.\n",
        "        width (int): Target width.\n",
        "        height (int): Target height.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Combined segmentation mask with class indices.\n",
        "    \"\"\"\n",
        "    combined_mask = np.zeros((height, width), dtype=np.uint8)  # Initialize with Background (0)\n",
        "\n",
        "    # Process non-Background masks first\n",
        "    for result in results:\n",
        "        label = result['label']\n",
        "        class_id = CLASS_MAPPING.get(label, 0)\n",
        "        if class_id == 0:  # Skip Background\n",
        "            continue\n",
        "        mask_array = decode_base64_mask(result['mask'], width, height)\n",
        "        combined_mask[mask_array > 0] = class_id\n",
        "\n",
        "    # Process Background last to ensure it doesn't overwrite other classes unnecessarily\n",
        "    # (Though the model usually provides non-overlapping masks for distinct classes other than background)\n",
        "    for result in results:\n",
        "        if result['label'] == 'Background':\n",
        "            mask_array = decode_base64_mask(result['mask'], width, height)\n",
        "            # Apply background only where no other class has been assigned yet\n",
        "            # This logic might need adjustment based on how the model defines 'Background'\n",
        "            # For this model, it seems safer to just let non-background overwrite it first.\n",
        "            # A simple application like this should be fine: if Background mask says pixel is BG, set it to 0.\n",
        "            # However, a more robust way might be to only set to background if combined_mask is still 0 (initial value)\n",
        "            combined_mask[mask_array > 0] = 0 # Class ID for Background is 0\n",
        "\n",
        "    return combined_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "single_image_intro_cell"
      },
      "source": [
        "## 4. Segmentation d'une Seule Image\n",
        "\n",
        "Avant de traiter toutes les images, concentrons-nous sur une seule pour bien comprendre le processus.\n",
        "\n",
        "Étapes :\n",
        "1.  Choisir une image.\n",
        "2.  Ouvrir l'image en mode binaire (`\"rb\"`) et lire son contenu (`data`).\n",
        "3.  Déterminer le `Content-Type` (par exemple, `\"image/jpeg\"` ou `\"image/png\"`).\n",
        "4.  Envoyer la requête POST à l'API avec `requests.post()` en passant l'URL, les headers et les données.\n",
        "5.  Vérifier le code de statut de la réponse. Une erreur sera levée si le code n'est pas 2xx (succès) grâce à `response.raise_for_status()`.\n",
        "6.  Convertir la réponse JSON en un dictionnaire Python avec `response.json()`.\n",
        "7.  Utiliser nos fonctions `get_image_dimensions` et `create_masks` pour obtenir le masque final.\n",
        "8.  Afficher l'image originale et le masque segmenté."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "single_image_code_cell"
      },
      "outputs": [],
      "source": [
        "if image_paths:\n",
        "    single_image_path = image_paths[0] # Prenons la première image de notre liste\n",
        "    print(f\"Traitement de l'image : {single_image_path}\")\n",
        "\n",
        "    try:\n",
        "        # Lire l'image en binaire\n",
        "        # Et mettez le contenu de l'image dans la variable image_data\n",
        "        image_data = None # A vous de jouer !\n",
        "\n",
        "        # Maintenant, utilisé l'API huggingface\n",
        "        # ainsi que les fonctions données plus haut pour ségmenter vos images.\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Une erreur est survenue : {e}\")\n",
        "else:\n",
        "    print(\"Aucune image à traiter. Vérifiez la configuration de 'image_dir' et 'max_images'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "batch_intro_cell"
      },
      "source": [
        "## 5. Segmentation de Plusieurs Images (Batch)\n",
        "\n",
        "Maintenant que nous savons comment traiter une image, nous pouvons créer une fonction pour en traiter plusieurs.\n",
        "Cette fonction va boucler sur la liste `image_paths` et appliquer la logique de segmentation à chaque image.\n",
        "Nous utiliserons `tqdm` pour avoir une barre de progression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch_code_cell"
      },
      "outputs": [],
      "source": [
        "def segment_images_batch(list_of_image_paths):\n",
        "    \"\"\"\n",
        "    Segmente une liste d'images en utilisant l'API Hugging Face.\n",
        "\n",
        "    Args:\n",
        "        list_of_image_paths (list): Liste des chemins vers les images.\n",
        "\n",
        "    Returns:\n",
        "        list: Liste des masques de segmentation (tableaux NumPy).\n",
        "              Contient None si une image n'a pas pu être traitée.\n",
        "    \"\"\"\n",
        "    batch_segmentations = []\n",
        "\n",
        "    # N'oubliez pas de mettre une pause entre chaque appel API !\n",
        "\n",
        "\n",
        "    return batch_segmentations\n",
        "\n",
        "# Appeler la fonction pour segmenter les images listées dans image_paths\n",
        "if image_paths:\n",
        "    print(f\"\\nTraitement de {len(image_paths)} image(s) en batch...\")\n",
        "    batch_seg_results = segment_images_batch(image_paths)\n",
        "    print(\"Traitement en batch terminé.\")\n",
        "else:\n",
        "    batch_seg_results = []\n",
        "    print(\"Aucune image à traiter en batch.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "display_batch_intro_cell"
      },
      "source": [
        "## 6. Affichage des Résultats en Batch\n",
        "\n",
        "Nous allons maintenant créer une fonction pour afficher les images originales et leurs segmentations correspondantes côte à côte, dans une grille."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "display_batch_code_cell"
      },
      "outputs": [],
      "source": [
        "def display_segmented_images_batch(original_image_paths, segmentation_masks):\n",
        "    \"\"\"\n",
        "    Affiche les images originales et leurs masques segmentés.\n",
        "\n",
        "    Args:\n",
        "        original_image_paths (list): Liste des chemins des images originales.\n",
        "        segmentation_masks (list): Liste des masques segmentés (NumPy arrays).\n",
        "    \"\"\"\n",
        "    # Matplotlib, ça vous parle ?\n",
        "    # Alors... au travail ! 😉\n",
        "\n",
        "# Afficher les résultats du batch\n",
        "if batch_seg_results:\n",
        "    display_segmented_images_batch(image_paths, batch_seg_results)\n",
        "else:\n",
        "    print(\"Aucun résultat de segmentation à afficher.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_cell"
      },
      "source": [
        "## Conclusion et Prochaines Étapes\n",
        "\n",
        "Félicitations ! Vous avez appris à :\n",
        "- Configurer les appels à l'API d'inférence Hugging Face.\n",
        "- Envoyer des images pour la segmentation.\n",
        "- Interpréter les résultats (avec l'aide des fonctions fournies).\n",
        "- Visualiser les segmentations.\n",
        "\n",
        "Pistes d'amélioration ou d'exploration :\n",
        "- **Gestion d'erreurs plus fine** : Implémenter des tentatives multiples (retry) en cas d'échec de l'API (par exemple, si le modèle est en cours de chargement).\n",
        "- **Appels asynchrones** : Pour un grand nombre d'images, des appels asynchrones (avec `asyncio` et `aiohttp`) seraient beaucoup plus rapides.\n",
        "- **Autres modèles** : Explorer d'autres modèles de segmentation ou d'autres tâches sur Hugging Face Hub.\n",
        "\n",
        "N'hésitez pas à modifier le code, à tester avec vos propres images et à explorer davantage !\n",
        "\n",
        "**_Note_** : Si vous aimez ce modèle, n'hésitez pas à le [télécharger](https://huggingface.co/sayeed99/segformer_b3_clothes) et jouer avec directement sur votre machine !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TV_ctytI7EQn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
